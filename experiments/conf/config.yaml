hydra:
  run:
    dir: ./logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

wandb:
  enable: False
  project: "multitask_personalization"
  entity: null
  group: null
  run_name: null
  dir: ./logs/wandb

video_dir: ${hydra:runtime.output_dir}/videos
model_dir: ${hydra:runtime.output_dir}/models
saved_state_dir: ${hydra:runtime.output_dir}/saved_states
train_results_file: ${hydra:runtime.output_dir}/train_results.csv
eval_results_file: ${hydra:runtime.output_dir}/eval_results.csv
config_file: ${hydra:runtime.output_dir}/config.yaml
record_train_videos: false
record_eval_videos: false
seed: 0
eval_seed_offset: 100000
train_logging_interval: 100
approach_name: ${hydra:runtime.choices.approach}
env_name: ${hydra:runtime.choices.env}
defaults:
  - _self_
  - approach: random
  - env: tiny
  - csp_solver: lifelong_random_walk
  - rom_model: spherical
  - llm: canned  # change to openai for final experiments
